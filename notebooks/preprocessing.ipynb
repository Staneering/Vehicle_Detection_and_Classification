{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c52160f1-d7ec-4729-8c91-d8604206f741",
   "metadata": {},
   "source": [
    "## **Data Preprocessing (HOG, HSV and LBP)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9464ada-b75e-4393-ba6b-17ed4c68346a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "293a3127-3906-4259-83ff-492edfe05bc5",
   "metadata": {},
   "source": [
    "## **Setup, import and paths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75b29e6a-4daa-469b-96fe-9c3f38e46b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready! VEHICLE_DIR = C:\\Users\\Glory\\ML Class\\Computer Vision\\Vehicle-Detection\\Data\\vehicle\n"
     ]
    }
   ],
   "source": [
    "# ─── Cell 1: Setup ────────────────────────────────────────────────\n",
    "import sys, os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import cv2\n",
    "from typing import List, Tuple\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "from src.config import (\n",
    "    VEHICLE_DIR, NON_VEHICLE_DIR,\n",
    "    HOG_FEATURE_PATH, HOG_LABEL_PATH\n",
    ")\n",
    "from src.preprocessing import (\n",
    "    fetch_and_sample_image_paths,\n",
    "    load_and_extract_features,               # HOG-only\n",
    "    load_and_extract_hog_hsv_features,       # HOG+HSV\n",
    "    load_and_extract_hsv_lbp_features        # HSV+LBP\n",
    ")\n",
    "\n",
    "print(\"Ready! VEHICLE_DIR =\", VEHICLE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "029e678d-54c9-4e2d-9f3f-91337de84e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Collected 1500 total samples\n",
      "Label distribution: Counter({'cars': 300, 'motorcycle': 300, 'threewheel': 300, 'trucks': 300, 'non-vehicle': 300})\n"
     ]
    }
   ],
   "source": [
    "# ─── Cell 2: Collect image paths & labels ─────────────────────────\n",
    "annotations = fetch_and_sample_image_paths(\n",
    "    VEHICLE_DIR,\n",
    "    NON_VEHICLE_DIR,\n",
    "    sample_size=300\n",
    ")\n",
    "\n",
    "print(f\"✔️ Collected {len(annotations)} total samples\")\n",
    "print(\"Label distribution:\", Counter(label for _, label in annotations))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139adf59-b600-40ad-98d4-ddfbbc5a6a2a",
   "metadata": {},
   "source": [
    "## **HOG Feature Extraction and HSV**\n",
    "\n",
    "Extract & Save HOG-only Features (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42e22554-b60f-41ee-8458-7465c492f647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ HOG-only feature matrix shape: (1500, 8100)\n",
      "✅ Saved HOG-only features and scaler\n"
     ]
    }
   ],
   "source": [
    "#Extract & Save HOG-only Features (SVM) ───────────────\n",
    "X_hog, y_hog = load_and_extract_features(annotations)\n",
    "print(\"✔️ HOG-only feature matrix shape:\", X_hog.shape)\n",
    "\n",
    "scaler_hog = StandardScaler()\n",
    "X_hog_scaled = scaler_hog.fit_transform(X_hog)\n",
    "\n",
    "# np.save(\"../Data/splits/hog_features.npy\", X_hog_scaled)\n",
    "# np.save(\"../Data/splits/hog_labels.npy\", y_hog)\n",
    "joblib.dump(scaler_hog, \"../model/hog_scaler.pkl\")\n",
    "print(\"✅ Saved HOG-only features and scaler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0f99ea4-b4cb-42ad-a1c8-87f99e884e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting HSV+LBP features: 100%|█████████████████████████████████████████████████| 1500/1500 [18:31<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ HSV+LBP feature matrix shape: (1500, 768)\n",
      "✅ Saved HSV+LBP features and scaler\n"
     ]
    }
   ],
   "source": [
    "#Extract & Save HSV+LBP Features ──────────────────────\n",
    "X_hsv_lbp, y_hsv_lbp = load_and_extract_hsv_lbp_features(annotations)\n",
    "print(\"✔️ HSV+LBP feature matrix shape:\", X_hsv_lbp.shape)\n",
    "\n",
    "scaler_hsv_lbp = StandardScaler()\n",
    "X_hsv_lbp_scaled = scaler_hsv_lbp.fit_transform(X_hsv_lbp)\n",
    "\n",
    "np.save(\"../Data/splits/hsv_lbp_features.npy\", X_hsv_lbp_scaled)\n",
    "np.save(\"../Data/splits/hsv_lbp_labels.npy\", y_hsv_lbp)\n",
    "\n",
    "joblib.dump(scaler_hsv_lbp, \"../model/hsv_lbp_scaler.pkl\")\n",
    "print(\"✅ Saved HSV+LBP features and scaler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44e5bae7-6a26-487b-aa1c-3e977f9d9d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting HOG+HSV features: 100%|█████████████████████████████████████████████████| 1500/1500 [02:34<00:00,  9.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Combined feature matrix shape: (1500, 8612)\n",
      "✅ Saved combined HOG+HSV features and scaler\n"
     ]
    }
   ],
   "source": [
    "#Extract & Save HOG+HSV Combined Features ─────────────\n",
    "X_combined, y_combined = load_and_extract_hog_hsv_features(annotations)\n",
    "print(\"✔️ Combined feature matrix shape:\", X_combined.shape)\n",
    "\n",
    "scaler_combined = StandardScaler()\n",
    "X_combined_scaled = scaler_combined.fit_transform(X_combined)\n",
    "\n",
    "np.save(HOG_FEATURE_PATH, X_combined_scaled)\n",
    "np.save(HOG_LABEL_PATH, y_combined)\n",
    "joblib.dump(scaler_combined, \"../model/cfeature_scaler.pkl\")\n",
    "print(\"✅ Saved combined HOG+HSV features and scaler\")\n",
    "\n",
    "\n",
    "joblib.dump(scaler_hsv_lbp, \"../model/hsv_hog_scaler.pkl\")\n",
    "print(\"✅ Saved HSV+LBP features and scaler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f93b5f2-af63-4038-a041-ba83ff25b957",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# np.save(HOG_FEATURE_PATH, X_scaled)\n",
    "# np.save(HOG_LABEL_PATH, y)\n",
    "# print(f\"✅ Saved scaled features to {HOG_FEATURE_PATH}\")\n",
    "# print(f\"✅ Saved labels to {HOG_LABEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592ed858-7adb-4aff-8513-611fa25bd63e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# np.save(HOG_FEATURE_PATH, X)\n",
    "# np.save(HOG_LABEL_PATH, y)\n",
    "\n",
    "# print(f\"✅ Saved features to {HOG_FEATURE_PATH}\")\n",
    "# print(f\"✅ Saved labels   to {HOG_LABEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332afaab-9f75-4a18-8d79-0c81f60a2533",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
